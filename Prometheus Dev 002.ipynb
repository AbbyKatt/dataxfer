{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prometheus - Knowledge based chatbot\n",
    "#Configurable backend for different chatbot engines\n",
    "\n",
    "#Steps: Build vector indexes\n",
    "#       Initiate chat engine (onfly)\n",
    "#       Chat via GRADIO\n",
    "\n",
    "#------------------------------------------------------------------------------------------------------------------------\n",
    "#Config - remove keys before checking in\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"MRXp5XCT3BlbkFJdHuVrZl8N6PV8fR6ojoQ\"\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = 'YHzwIvgFejEmLIbwbZPFSG'\n",
    "#------------------------------------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ConstructLLM - gets the selected LLM interface\n",
    "#make a \"const\" of HuggingFace_Local, HuggingFace_Hub, OpenAI\n",
    "\n",
    "from langchain import HuggingFacePipeline\n",
    "from llama_index import SimpleDirectoryReader, GPTVectorStoreIndex, PromptHelper, LLMPredictor\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.llms import HuggingFaceHub\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index import LangchainEmbedding, ServiceContext\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from accelerate import init_empty_weights, infer_auto_device_map\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "#Loads a huggingface model locally and pushes it to GPU (CUDA)\n",
    "def HuggingFace_Local(hfSubModel):\n",
    "    model_name =  hfSubModel #'lmsys/fastchat-t5-3b-v1.0'\n",
    "    config = T5Config.from_pretrained(model_name )\n",
    "    with init_empty_weights():\n",
    "        model_layer = T5ForConditionalGeneration(config=config)\n",
    "    device_map = infer_auto_device_map(model_layer,\n",
    "                                    max_memory={0: \"12GiB\",1: \"12GiB\", \"cpu\": \"0GiB\"},\n",
    "                                    no_split_module_classes=[\"T5Block\"])\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name,torch_dtype=torch.float16,device_map=device_map,offload_folder=\"offload\",offload_state_dict=True)    \n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    pipe = pipeline(\"text2text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            device = 0, \n",
    "            max_length=1536,\n",
    "            temperature=0,\n",
    "            top_p = 1,\n",
    "            num_beams=1,\n",
    "            early_stopping=True \n",
    "        )\n",
    "\n",
    "    llm = HuggingFacePipeline(pipeline=pipe)\n",
    "    return llm\n",
    "\n",
    "#HuggingFace Prototyping REST endpoints\n",
    "def HuggingFace_Hub(hfSubModel):\n",
    "    llm = HuggingFaceHub(\n",
    "            repo_id=hfSubModel,\n",
    "        model_kwargs={'temperature':0.3}\n",
    "    )\n",
    "    return llm\n",
    "\n",
    "#Classic ChatGPT\n",
    "def OpenAIAPI(subModelNotUsed):\n",
    "    num_outputs = 512\n",
    "    llm=ChatOpenAI(temperature=0.7, model_name=\"gpt-3.5-turbo\") #max_tokens=num_outputs\n",
    "    return llm\n",
    "\n",
    "#Contructor for LLM\n",
    "AITypes=[\"HuggingFace_Local\", \"HuggingFace_Hub\", \"OpenAI\"]\n",
    "SubModels=[\"EleutherAI/gpt-neo-2.7B\",\"lmsys/fastchat-t5-3b-v1.0\",\"tiiuae/falcon-7b-instruct\"]\n",
    "def ConstructLLMInteface(aiType = \"HuggingFace_Local\",hfSubModel=\"lmsys/fastchat-t5-3b-v1.0\"):\n",
    "    if aiType == \"HuggingFace_Local\":\n",
    "        return HuggingFace_Local(hfSubModel)\n",
    "    elif aiType == \"HuggingFace_Hub\":\n",
    "        return HuggingFace_Hub(hfSubModel)\n",
    "    elif aiType == \"OpenAI\":\n",
    "        return OpenAIAPI(hfSubModel)\n",
    "    else:\n",
    "        raise Exception(\"Invalid AI Type: \" + aiType + \" - supported types are: \" + str(AITypes))\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Index Construction - either OpenAI or HuggingFace\n",
    "#From: https://levelup.gitconnected.com/connecting-chatgpt-with-your-own-data-using-llamaindex-663844c06653\n",
    "from llama_index import SimpleDirectoryReader, GPTListIndex, GPTVectorStoreIndex, LLMPredictor, PromptHelper,StorageContext, load_index_from_storage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.llms import HuggingFaceHub\n",
    "import gradio as gr\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "#Build an index using OPENAI to produce the embeddings\n",
    "def OPENAI_ConstructIndex(directory_path,llm_predictor):\n",
    "    max_input_size = 4096\n",
    "    num_outputs = 512\n",
    "    max_chunk_overlap = 20\n",
    "    chunk_size_limit = 600\n",
    "    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)\n",
    "    documents = SimpleDirectoryReader(directory_path).load_data()\n",
    "    \n",
    "    #LLM passed in from constructor\n",
    "    #llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.7, model_name=\"gpt-3.5-turbo\", max_tokens=num_outputs))\n",
    "    index = GPTVectorStoreIndex.from_documents(documents,llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
    "    index.storage_context.persist(persist_dir=\"IDX_OpenAI\")\n",
    "    return index\n",
    "\n",
    "def HF_ConstructIndex(directory_path,llm_predictor):\n",
    "    max_input_size = 2048\n",
    "    num_outputs = 512\n",
    "    max_chunk_overlap = 20\n",
    "    chunk_size_limit = 300\n",
    "    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap)\n",
    "    \n",
    "    #Same hugging face embeddings model no matter which LLM model used\n",
    "    embed_model = LangchainEmbedding(HuggingFaceEmbeddings())\n",
    "    service_context = ServiceContext.from_defaults(embed_model=embed_model, llm_predictor=LLMPredictor(llm_predictor), prompt_helper=prompt_helper, chunk_size_limit=chunk_size_limit)\n",
    "\n",
    "    # build index\n",
    "    documents = SimpleDirectoryReader(directory_path).load_data()\n",
    "    new_index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "    new_index.storage_context.persist(persist_dir=\"IDX_HuggingFace\")\n",
    "\n",
    "    return new_index\n",
    "\n",
    "\n",
    "    # llm = HuggingFaceHub(\n",
    "    #         #repo_id='google/flan-t5-large',\n",
    "    #         repo_id='lmsys/fastchat-t5-3b-v1.0',\n",
    "    #     model_kwargs={'temperature':0.3}\n",
    "    # )\n",
    "\n",
    "    # #Prediction Model Selection---------------------------------------------------------------\n",
    "\n",
    "    # embed_model = LangchainEmbedding(HuggingFaceEmbeddings())\n",
    "\n",
    "    # # set maximum input size\n",
    "    # max_input_size = 2048\n",
    "    # # set number of output tokens\n",
    "    # num_outputs = 512\n",
    "    # # set maximum chunk overlap\n",
    "    # max_chunk_overlap = 20\n",
    "    # # set chunk size limit\n",
    "    # chunk_size_limit = 300\n",
    "    # prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap)\n",
    "\n",
    "    # service_context = ServiceContext.from_defaults(embed_model=embed_model, llm_predictor=LLMPredictor(llm), prompt_helper=prompt_helper, chunk_size_limit=chunk_size_limit)\n",
    "\n",
    "    # # build index\n",
    "    # documents = SimpleDirectoryReader('Docs').load_data()\n",
    "\n",
    "    # new_index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "\n",
    "\n",
    "    # query_engine = new_index.as_query_engine(\n",
    "    #     verbose=True,\n",
    "    #     similarity_top_k=2\n",
    "    # )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test Code (non-interactive)--------------------------------------------------------------------------------------------------------\n",
    "\n",
    "#Create AI\n",
    "# llm=ConstructLLMInteface(\"HuggingFace_Local\",\"lmsys/fastchat-t5-3b-v1.0\")\n",
    "\n",
    "# #Build Index\n",
    "# index=HF_ConstructIndex(\"Docs\",llm_predictor=llm)\n",
    "\n",
    "llm=ConstructLLMInteface(\"OpenAI\",\"\")\n",
    "\n",
    "#Build Index\n",
    "index=HF_ConstructIndex(\"Docs\",llm_predictor=llm)\n",
    "\n",
    "prompt=\"What were his accomplishments?\"\n",
    "response = index.as_query_engine().query(prompt)\n",
    "print(response.response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Web Interface-----------------------------------------------------------------------------------------------------------------------\n",
    "import gradio as gr\n",
    "global llm\n",
    "global index\n",
    "\n",
    "# Mock actions\n",
    "def build_indexes(input, model,docfolder):\n",
    "    global llm\n",
    "    global index\n",
    "    #Initiate model and build indexes    \n",
    "    llm=ConstructLLMInteface(model)\n",
    "    index=HF_ConstructIndex(docfolder,llm_predictor=llm)\n",
    "    return \"Indexes built successfully using \" + model\n",
    "\n",
    "def chat(input, model):\n",
    "    global llm\n",
    "    global index\n",
    "    response = index.as_query_engine().query(input)\n",
    "    return response.response        \n",
    "\n",
    "def main_interface(input, model, docfolder, action):\n",
    "    if action == \"Build Indexes\":\n",
    "        return build_indexes(input,model,docfolder)\n",
    "    elif action == \"Chat\":\n",
    "        return chat(input, model)\n",
    "\n",
    "sources=[\"Docs\",\"DocsCountries\"]\n",
    "iface = gr.Interface(\n",
    "    fn=main_interface, \n",
    "    inputs=[\n",
    "        gr.components.Textbox(lines=2, label=\"Input\",value=\"What were his accomplishments?\"),\n",
    "        gr.components.Dropdown(choices=AITypes, label=\"AI Model\",value=AITypes[0]),\n",
    "        gr.components.Dropdown(choices=sources, label=\"Document Folder\",value=sources[0]),\n",
    "        gr.components.Radio(choices=['Build Indexes', 'Chat'], label=\"Action\",value=\"Build Indexes\")\n",
    "    ], \n",
    "    outputs=\"text\",\n",
    "    title=\"Knowledge Chatbot\",\n",
    "    description=\"Select an AI model and an action to perform. For 'Chat', enter a message to get the model's response.\"\n",
    ")\n",
    "\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zzz------------------------------------------------------------------------------------------------------------------------------\n",
    "#OpenAI Chatbot - note no direct reference to model or API (higher order API)\n",
    "\n",
    "def chatbot(input_text):\n",
    "    #index = GPTVectorStoreIndex.load_from_disk('index.json')\n",
    "\n",
    "    storage_context = StorageContext.from_defaults(persist_dir=\"IDX\")\n",
    "    # load index\n",
    "    index = load_index_from_storage(storage_context)\n",
    "\n",
    "    response=index.as_query_engine().query(input_text)\n",
    "\n",
    "    #response = index.query(input_text, response_mode=\"compact\")\n",
    "    return response.response\n",
    "\n",
    "iface = gr.Interface(fn=chatbot,\n",
    "                     inputs=gr.components.Textbox(lines=7, label=\"Enter your text\"),\n",
    "                     outputs=\"text\",\n",
    "                     title=\"Custom-trained AI Chatbot\")\n",
    "\n",
    "iface.launch(share=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zzz------------------------------------------------------------------------------------------------------------------------------\n",
    "#From: https://levelup.gitconnected.com/connecting-chatgpt-with-your-own-data-using-llamaindex-663844c06653\n",
    "from llama_index import StorageContext, load_index_from_storage\n",
    "\n",
    "#Vector store index (index in this case)\n",
    "#https://gpt-index.readthedocs.io/en/latest/reference/indices/vector_store.html\n",
    "\n",
    "#https://gpt-index.readthedocs.io/en/latest/reference/service_context.html#llama_index.indices.service_context.ServiceContext\n",
    "\n",
    "#service_context = service_context\n",
    "\n",
    "# def my_chatGPT_bot(input_text):\n",
    "    # load the index from vector_store.json\n",
    "storage_context = StorageContext.from_defaults(persist_dir=\".\")\n",
    "index = load_index_from_storage(storage_context)\n",
    "index\n",
    "\n",
    "    # # create a query engine to ask question\n",
    "    # query_engine = index.as_query_engine()\n",
    "    # response = query_engine.query(input_text)\n",
    "    # return response.response\n",
    "  \n",
    "\n",
    "#my_chatGPT_bot(\"Hello world!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zzz------------------------------------------------------------------------------------------------------------------------------\n",
    "from llama_index import SimpleDirectoryReader, GPTVectorStoreIndex, PromptHelper, LLMPredictor\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from llama_index import LangchainEmbedding, ServiceContext\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, T5Config\n",
    "from accelerate import init_empty_weights, infer_auto_device_map\n",
    "import torch\n",
    "import os\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'\n",
    "\n",
    "# model_name = 'lmsys/fastchat-t5-3b-v1.0'\n",
    "\n",
    "# config = T5Config.from_pretrained(model_name )\n",
    "# with init_empty_weights():\n",
    "#     model_layer = T5ForConditionalGeneration(config=config)\n",
    "\n",
    "# device_map = infer_auto_device_map(model_layer,\n",
    "#                                 max_memory={0: \"12GiB\",1: \"12GiB\", \"cpu\": \"0GiB\"},\n",
    "#                                 no_split_module_classes=[\"T5Block\"])\n",
    "\n",
    "# # the value for device_map = {'': 0}, i.e. loading the entire Model on 1st GPU\n",
    "# model = T5ForConditionalGeneration.from_pretrained(model_name,\n",
    "#                                                    torch_dtype=torch.float16,\n",
    "#                                                    device_map=device_map,\n",
    "#                                                    offload_folder=\"offload\",\n",
    "#                                                    offload_state_dict=True)\n",
    "\n",
    "# tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# from transformers import pipeline\n",
    "\n",
    "# pipe = pipeline(\n",
    "#     \"text2text-generation\", \n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     device = 0, \n",
    "#     max_length=1536,\n",
    "#     temperature=0,\n",
    "#     top_p = 1,\n",
    "#     num_beams=1,\n",
    "#     early_stopping=True \n",
    "#     )\n",
    "\n",
    "#llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "#Prediction Model Selection---------------------------------------------------------------\n",
    "\n",
    "#Local Host (GPU!)\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.llms import HuggingFaceHub\n",
    "\n",
    "\n",
    "\n",
    "#HuggingFace Prototype AI!\n",
    "llm = HuggingFaceHub(\n",
    "        #repo_id='google/flan-t5-large',\n",
    "        repo_id='lmsys/fastchat-t5-3b-v1.0',\n",
    "    model_kwargs={'temperature':0.3}\n",
    ")\n",
    "\n",
    "#Prediction Model Selection---------------------------------------------------------------\n",
    "\n",
    "embed_model = LangchainEmbedding(HuggingFaceEmbeddings())\n",
    "\n",
    "# set maximum input size\n",
    "max_input_size = 2048\n",
    "# set number of output tokens\n",
    "num_outputs = 512\n",
    "# set maximum chunk overlap\n",
    "max_chunk_overlap = 20\n",
    "# set chunk size limit\n",
    "chunk_size_limit = 300\n",
    "prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(embed_model=embed_model, llm_predictor=LLMPredictor(llm), prompt_helper=prompt_helper, chunk_size_limit=chunk_size_limit)\n",
    "\n",
    "# build index\n",
    "documents = SimpleDirectoryReader('Docs').load_data()\n",
    "\n",
    "new_index = GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n",
    "\n",
    "\n",
    "query_engine = new_index.as_query_engine(\n",
    "    verbose=True,\n",
    "    similarity_top_k=2\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTorchCUDA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
